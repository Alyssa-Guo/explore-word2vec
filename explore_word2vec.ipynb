{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "from gensim.models import word2vec\n",
    "import gensim\n",
    "import re\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanText(text):\n",
    "\t#assume all reveiws written in english, delete all non-ascii char\n",
    "\ttext = text.encode('ascii','ignore').decode()\n",
    "\t\n",
    "\t#delete HTML tag\n",
    "\ttext = re.sub(r'</?\\w+[^>]*>','',text)\n",
    "\t\n",
    "\t#delete punctuation except char'char case(e.g. \"haven't\",\"can't\",\"macy's\")\n",
    "\ttext = re.sub(\" '|'\\W|[-(),.\\\"!?#*$~`\\{\\}\\[\\]/+&*=:^]\", \" \", text)\n",
    "\t\t\n",
    "\t#transform several space into one space\n",
    "\ttext = re.sub(\"\\s+\", \" \", text)\n",
    "\t\t\n",
    "\t#transform all letters to lowercase\n",
    "\ttext = text.lower().split()\n",
    "\n",
    "\treturn text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createInput(neg_path, pos_path):\n",
    "\t#\n",
    "\t#Gensim's word2vec input format is a list of lists, each list inside the list indicates a review. \n",
    "\t#[['word1', 'word2', 'word3', '...'],['word1', 'word2', '...'], ..., ['...','...']]\n",
    "\t#\n",
    "\n",
    "\tprint(\"Loading the imdb reviews data and clean the data\")\n",
    "\tneg_files = glob.glob(neg_path + \"/*.txt\")\n",
    "\tpos_files = glob.glob(pos_path + \"/*.txt\")\n",
    "\t\n",
    "\tsentences = []\n",
    "\t\n",
    "\tfor tnf in neg_files:\n",
    "\t\tf = open(tnf, 'r', errors='replace')\n",
    "\n",
    "\t\tline = f.read()\n",
    "\n",
    "\t\t#clean the data by delete punctuations and transform all uppercase to lowercase\n",
    "\t\tclean_line = cleanText(line)\n",
    "\t\t\n",
    "\t\tsentences.append(clean_line)\n",
    "\t\t\n",
    "\t\tf.close()\n",
    "\t\n",
    "\tfor tpf in pos_files:\n",
    "\t\tf = open(tpf, 'r', errors='replace')\n",
    "\t\tline = f.read()\n",
    "\t\tclean_line = cleanText(line)\n",
    "\t\tsentences.append(clean_line)\n",
    "\t\tf.close()\n",
    "\t\n",
    "\tprint(\"Data loaded and cleaned.\")\n",
    "\n",
    "\treturn sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def trainModel(sentences):\n",
    "\t#train word vector\n",
    "\tprint(\"train word vector\")\n",
    "\tlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "\t\tlevel=logging.INFO)\n",
    "\n",
    "\t#set values for the parameters in Word2Vec\n",
    "\tprint(\"parameters of training the model\")\n",
    "\tprint(\"dimension: word vector dimensionality\")\n",
    "\tdimension = 200  \n",
    "\tprint(\"min_count: any word that does not occur at least this many times across all documents is ignored\")\n",
    "\tmin_count = 5\n",
    "\tprint(\"num_worders: number of threads to run in parallel\")\n",
    "\tnum_workers = 4\n",
    "\tprint(\"window size\")  \n",
    "\twindow_size = 5\n",
    "\t#downsample setting for frequent words  \n",
    "\tdownsampling = 1e-3  \n",
    "\n",
    "\tprint(\"Training model\")\n",
    "\tmodel = word2vec.Word2Vec(sentences, workers=num_workers,\n",
    "\t\t\t\t\t\t\t  size=dimension, min_count=min_count,\n",
    "\t\t\t\t\t\t\t  window=window_size, sample=downsampling, sg = 1)\n",
    "\n",
    "\t#\n",
    "\t#If finished training a model (no more updates, only querying), \n",
    "\t#could do:\n",
    "\t# model.init_sims(replace=True)\n",
    "\t#to trim unneeded model memory = use (much) less RAM.\n",
    "\t#\n",
    "\n",
    "\tprint(\"save the word vector model to disk\")\n",
    "\t#specify path and model's name\n",
    "\tpath = \"/Users/ziluguo/Desktop/try/\"\n",
    "\tfname = \"wordVectorModel\"\n",
    "\tmodel.save(path+fname)\n",
    "\n",
    "\t#train phrases model\n",
    "\tdimension = 200\n",
    "\tprint(\"train phrases model, word vector's size is %d\" %dimension)\n",
    "\tbigram_transformer = gensim.models.Phrases(sentences)\n",
    "\tmodel = word2vec.Word2Vec(bigram_transformer[sentences], workers=num_workers,\n",
    "\t\t\t\t\t\t\t  size=dimension, min_count=min_count,\n",
    "\t\t\t\t\t\t\t  window=window_size, sample=downsampling, sg = 1)\n",
    "\n",
    "\tprint(\"save the phrases vector model to disk\")\n",
    "\t#specify path and model's name\n",
    "\tpath = \"/Users/ziluguo/Desktop/try/\"\n",
    "\tfname = \"phrasesVectorModel\"\n",
    "\tmodel.save(path+fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def explore(path):\n",
    "\t#load word vector model\n",
    "\tprint(\"Load Word Vector Model\")\n",
    "\tfname = \"wordVectorModel\"\n",
    "\tmodel = word2vec.Word2Vec.load(path+fname)\n",
    "\t\n",
    "\t#number of words, number of features\n",
    "\tprint(\"show words vector's shape\")\n",
    "\tprint(model.syn0.shape)\n",
    "\n",
    "\tprint(\"access individual word vector\")\n",
    "\tprint(\"for example, the word vector of 'best'\")\n",
    "\tprint(model[\"best\"])\n",
    "\n",
    "\tprint(\"show the word that is most dissimilar from the others\")\n",
    "\tword_set = \"best worst fine london\"\n",
    "\tmost_dissimilar = model.doesnt_match(word_set.split()) + '\\n'\n",
    "\tprint(\"for example, words are %s, the most dissimilar word is %s\" %(word_set, most_dissimilar))\n",
    "\n",
    "\tprint(\"show the word that is most dissimilar from the others\")\n",
    "\tword_set = \"best finest refreshing poorest\"\n",
    "\tmost_dissimilar = model.doesnt_match(word_set.split()) + '\\n'\n",
    "\tprint(\"for example, words are %s, the most dissimilar word is %s\" %(word_set, most_dissimilar))\n",
    "\n",
    "\tprint(\"show the word that is most dissimilar from the others\")\n",
    "\tword_set = \"enjoy apple juice orange\"\n",
    "\tmost_dissimilar = model.doesnt_match(word_set.split()) + '\\n'\n",
    "\tprint(\"for example, words are %s, the most dissimilar word is %s\" %(word_set, most_dissimilar))\t\n",
    "    \n",
    "\t#show similarity\n",
    "\tprint(\"Show most similar words and corresponding distance to the given words\")\n",
    "\t#indicate positive words\n",
    "\tpos = ['best']\n",
    "\t#doesn't indicate nagative word indicate negative words\n",
    "\tprint(\"for example, the given word is %s\" %(pos))\n",
    "\tmost_similar = model.most_similar(positive=pos)\n",
    "\tprint(most_similar)\n",
    "\n",
    "\t#show similarity\n",
    "\tprint(\"Show most similar words and corresponding distance to the given words\")\n",
    "\t#indicate positive words\n",
    "\tpos = ['worst']\n",
    "\t#doesn't indicate nagative word indicate negative words\n",
    "\tprint(\"for example, the given word is %s\" %(pos))\n",
    "\tmost_similar = model.most_similar(positive=pos)\n",
    "\tprint(most_similar)\n",
    "    \n",
    "\t#do more complex queries like analogies such as: king - man + woman = queen \n",
    "\tprint(\"Show most similar words and corresponding distance to the given analogies\")\n",
    "\t#indicate positive words\n",
    "\tpos = ['best', 'refreshing']\n",
    "\t#indicate negative words\n",
    "\tneg = ['worst']\n",
    "\tprint(\"for example, the given positive words are %s, the given negative word is %s\" %(pos, neg))\n",
    "\tmost_similar = model.most_similar(positive=pos, negative=neg)\n",
    "\tprint(most_similar)\n",
    "    \n",
    "\t#do more complex queries like analogies such as: king - man + woman = queen \n",
    "\tprint(\"Show most similar words and corresponding distance to the given analogies\")\n",
    "\t#indicate positive words\n",
    "\tpos = ['worst', 'poorest']\n",
    "\t#indicate negative words\n",
    "\tneg = ['best']\n",
    "\tprint(\"for example, the given positive words are %s, the given negative word is %s\" %(pos, neg))\n",
    "\tmost_similar = model.most_similar(positive=pos, negative=neg)\n",
    "\tprint(most_similar)\n",
    "    \n",
    "\tprint(\"test model's accuracy here\")\n",
    "\t#test the quality of the word vectors\n",
    "\t#The accuracy depends heavily on the amount of the training data.\n",
    "\t#read the evaluation file, get it at:\n",
    "\t#https://code.google.com/archive/p/word2vec/source/default/source\n",
    "\tquestions = \"questions-words.txt\"\n",
    "\tevals = open(path+questions, 'r').readlines()\n",
    "\tnum_sections = len([l for l in evals if l.startswith(':')])\n",
    "\tprint('total test sentences: {} '.format(len(evals) - num_sections))\n",
    "\t#test accuracy of model\n",
    "\taccuracy = model.accuracy(path+questions)\n",
    "\tsum_corr = len(accuracy[-1]['correct'])\n",
    "\tsum_incorr = len(accuracy[-1]['incorrect'])\n",
    "\ttotal = sum_corr + sum_incorr\n",
    "\tpercent = lambda a: a / total * 100\n",
    "\tprint('Total evaluation sentences: {}, Correct: {:.2f}%, Incorrect: {:.2f}%'.format(total, percent(sum_corr), percent(sum_incorr)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explore the model\n",
      "Load Word Vector Model\n",
      "show words vector's shape\n",
      "(30321, 200)\n",
      "access individual word vector\n",
      "for example, the word vector of 'best'\n",
      "[ 0.13880603  0.0178613  -0.02573948 -0.14831917 -0.15835425  0.16883919\n",
      "  0.11967301  0.17873153  0.12080888  0.18553962 -0.03115853 -0.0314556\n",
      " -0.15939252 -0.07899953 -0.12812638  0.05657555 -0.32958868 -0.0049634\n",
      "  0.54324406  0.26949298 -0.42546371  0.49630183  0.01495651 -0.05225568\n",
      " -0.09862857  0.21544912 -0.28073025 -0.10606798 -0.23924465  0.04499689\n",
      "  0.28453296  0.0578398  -0.52040863 -0.500314    0.26657689 -0.36671656\n",
      " -0.02926834  0.53173608  0.43211129 -0.10846587  0.10721705  0.18494959\n",
      " -0.04721029 -0.143896   -0.18683045  0.07056804  0.08171466  0.271229\n",
      " -0.008992   -0.02410933 -0.41038242  0.29807949 -0.26044142 -0.17478347\n",
      "  0.18051535 -0.3792313  -0.14406045  0.26673809  0.26555544 -0.05673116\n",
      " -0.17148805 -0.06623498 -0.18849255 -0.11391533 -0.17411296  0.39583892\n",
      "  0.36459151 -0.13401714 -0.10806058  0.19026239 -0.07934557 -0.07115291\n",
      "  0.34832063 -0.18074708 -0.26971129 -0.23554759  0.19100063 -0.1809352\n",
      " -0.06124423 -0.15140928  0.0292105   0.09224248  0.2027889   0.33438003\n",
      "  0.49918801 -0.10801835  0.31492531  0.20739011 -0.06041195 -0.50459719\n",
      "  0.31057429  0.1384279   0.08431463 -0.01864159  0.0352626  -0.365435\n",
      "  0.3003144   0.13501951 -0.26184475  0.25615069 -0.23565498 -0.24473087\n",
      "  0.20663914  0.37101772 -0.10100286  0.18025796 -0.33563894  0.19429386\n",
      " -0.39850211 -0.08324438 -0.3464734   0.47256526  0.23075293  0.28838304\n",
      " -0.19652453 -0.10658205  0.52704376 -0.28726277  0.01184181 -0.14384581\n",
      " -0.13584511 -0.24354954 -0.14785461  0.31542799 -0.44956347 -0.23762336\n",
      "  0.04499418 -0.14078912 -0.38548085 -0.25600165 -0.12308372 -0.34498274\n",
      " -0.01261009  0.3969779  -0.60546416 -0.03414429 -0.12076472 -0.13688301\n",
      " -0.11724188 -0.01199779 -0.18506286 -0.37632221 -0.28878903 -0.11904757\n",
      "  0.61514646  0.27459577  0.05105711 -0.04510029  0.21002124 -0.03505103\n",
      "  0.28176308  0.23420931 -0.03154226 -0.34860513 -0.00450244  0.46417165\n",
      " -0.0443802   0.12744163  0.01210896 -0.20914516  0.29554254  0.05236155\n",
      "  0.14746097 -0.09629897 -0.07547246 -0.02809164 -0.18929572 -0.1954921\n",
      "  0.20908031 -0.30754155 -0.01993165 -0.35646948  0.20409489  0.19237202\n",
      "  0.04174547 -0.40395689 -0.00287665 -0.33532852  0.23010875  0.21952368\n",
      "  0.34132963  0.12937036  0.0452058  -0.64766449  0.46436092  0.71168381\n",
      "  0.02780569  0.47580898 -0.59003168  0.00187388 -0.09008065 -0.28441417\n",
      " -0.15132856  0.22137263 -0.04556316 -0.35528949 -0.10466058  0.23705587\n",
      " -0.19665603 -0.2455868 ]\n",
      "show the word that is most dissimilar from the others\n",
      "for example, words are best worst fine london, the most dissimilar word is london\n",
      "\n",
      "show the word that is most dissimilar from the others\n",
      "for example, words are best finest refreshing poorest, the most dissimilar word is refreshing\n",
      "\n",
      "show the word that is most dissimilar from the others\n",
      "for example, words are enjoy apple juice orange, the most dissimilar word is enjoy\n",
      "\n",
      "Show most similar words and corresponding distance to the given words\n",
      "for example, the given word is ['best']\n",
      "[('finest', 0.714893102645874), ('worst', 0.67093825340271), ('greatest', 0.6461453437805176), ('funniest', 0.6398975849151611), ('strongest', 0.6273582577705383), ('poorest', 0.6238601207733154), ('weakest', 0.6215401291847229), ('strangest', 0.5886249542236328), ('sexiest', 0.5843579769134521), ('wittiest', 0.5820038318634033)]\n",
      "Show most similar words and corresponding distance to the given words\n",
      "for example, the given word is ['worst']\n",
      "[('funniest', 0.7190247178077698), ('stupidest', 0.7163616418838501), ('dumbest', 0.702324628829956), ('cheesiest', 0.6821668744087219), ('scariest', 0.6752421855926514), ('best', 0.67093825340271), ('poorest', 0.6692594289779663), ('weirdest', 0.6615262031555176), ('greatest', 0.6583712697029114), ('lamest', 0.6512914299964905)]\n",
      "Show most similar words and corresponding distance to the given analogies\n",
      "for example, the given positive words are ['best', 'refreshing'], the given negative word is ['worst']\n",
      "[('unique', 0.5520704984664917), ('engrossing', 0.5384328365325928), ('insightful', 0.5377101898193359), ('fascinating', 0.526489794254303), ('fresh', 0.5262725353240967), ('truthful', 0.5200850963592529), ('lighthearted', 0.51930832862854), ('brooding', 0.5185928344726562), ('delightful', 0.5181107521057129), ('rewarding', 0.5135464668273926)]\n",
      "Show most similar words and corresponding distance to the given analogies\n",
      "for example, the given positive words are ['worst', 'poorest'], the given negative word is ['best']\n",
      "[('stupidest', 0.7152367234230042), ('cheesiest', 0.7139892578125), ('lamest', 0.7120643854141235), ('silliest', 0.7042285203933716), ('dumbest', 0.7005338668823242), ('weirdest', 0.6624621748924255), ('dullest', 0.6584751009941101), ('sickest', 0.652555525302887), ('strangest', 0.6509111523628235), ('crappiest', 0.6482438445091248)]\n",
      "test model's accuracy here\n",
      "total test sentences: 19544 \n",
      "Total evaluation sentences: 9917, Correct: 14.95%, Incorrect: 85.05%\n"
     ]
    }
   ],
   "source": [
    "#main\n",
    "#specify the path of datasets\n",
    "neg_path = \"/Users/ziluguo/Desktop/try/aclImdb/train/neg\"\n",
    "pos_path = \"/Users/ziluguo/Desktop/try/aclImdb/train/pos\"\n",
    "\n",
    "#use training data create input of word2vector\n",
    "sentences = createInput(neg_path, pos_path)\n",
    "\n",
    "#\n",
    "#What word2vec does is to represent each word as a vector in a form \n",
    "#that reflects how close words occur. \n",
    "#The closer the words tend to occur in a text the more similar the vectors will be.\n",
    "#\n",
    "\n",
    "#use word2vec train model\n",
    "trainModel(sentences)\n",
    "\n",
    "#display what model can do\n",
    "print(\"explore the model\")\n",
    "#model's path\n",
    "path = \"/Users/ziluguo/Desktop/try/\"\n",
    "explore(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
