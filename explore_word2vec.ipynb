{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "from gensim.models import word2vec\n",
    "import gensim\n",
    "import re\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanText(text):\n",
    "\t#assume all reveiws written in english, delete all non-ascii char\n",
    "\ttext = text.encode('ascii','ignore').decode()\n",
    "\t\n",
    "\t#delete HTML tag\n",
    "\ttext = re.sub(r'</?\\w+[^>]*>','',text)\n",
    "\t\n",
    "\t#delete punctuation except char'char case(e.g. \"haven't\",\"can't\",\"macy's\")\n",
    "\ttext = re.sub(\" '|'\\W|[-(),.\\\"!?#*$~`\\{\\}\\[\\]/+&*=:^]\", \" \", text)\n",
    "\t\t\n",
    "\t#transform several space into one space\n",
    "\ttext = re.sub(\"\\s+\", \" \", text)\n",
    "\t\t\n",
    "\t#transform all letters to lowercase\n",
    "\ttext = text.lower().split()\n",
    "\n",
    "\treturn text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createInput(neg_path, pos_path):\n",
    "\t#\n",
    "\t#Gensim's word2vec input format is a list of lists, each list inside the list indicates a review. \n",
    "\t#[['word1', 'word2', 'word3', '...'],['word1', 'word2', '...'], ..., ['...','...']]\n",
    "\t#\n",
    "\n",
    "\tprint(\"Loading the imdb reviews data and clean the data\")\n",
    "\tneg_files = glob.glob(neg_path + \"/*.txt\")\n",
    "\tpos_files = glob.glob(pos_path + \"/*.txt\")\n",
    "\t\n",
    "\tsentences = []\n",
    "\t\n",
    "\tfor tnf in neg_files:\n",
    "\t\tf = open(tnf, 'r', errors='replace')\n",
    "\n",
    "\t\tline = f.read()\n",
    "\n",
    "\t\t#clean the data by delete punctuations and transform all uppercase to lowercase\n",
    "\t\tclean_line = cleanText(line)\n",
    "\t\t\n",
    "\t\tsentences.append(clean_line)\n",
    "\t\t\n",
    "\t\tf.close()\n",
    "\t\n",
    "\tfor tpf in pos_files:\n",
    "\t\tf = open(tpf, 'r', errors='replace')\n",
    "\t\tline = f.read()\n",
    "\t\tclean_line = cleanText(line)\n",
    "\t\tsentences.append(clean_line)\n",
    "\t\tf.close()\n",
    "\t\n",
    "\tprint(\"Data loaded and cleaned.\")\n",
    "\n",
    "\treturn sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainModel(sentences):\n",
    "\t#train word vector\n",
    "\tprint(\"train word vector\")\n",
    "\tlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "\t\tlevel=logging.INFO)\n",
    "\n",
    "\t#set values for the parameters in Word2Vec\n",
    "    print(\"parameters of training the model\")\n",
    "\tprint(\"dimension: word vector dimensionality\")\n",
    "\tdimension = 200  \n",
    "\tprint(\"min_count: any word that does not occur at least this many times across all documents is ignored\")\n",
    "\tmin_count = 5\n",
    "\tprint(\"num_worders: number of threads to run in parallel\")\n",
    "\tnum_workers = 4\n",
    "\tprint(\"window size\")  \n",
    "\twindow_size = 5\n",
    "\t#downsample setting for frequent words  \n",
    "\tdownsampling = 1e-3  \n",
    "\n",
    "\tprint(\"Training model\")\n",
    "\tmodel = word2vec.Word2Vec(sentences, workers=num_workers,\n",
    "\t\t\t\t\t\t\t  size=dimension, min_count=min_count,\n",
    "\t\t\t\t\t\t\t  window=window_size, sample=downsampling, sg = 1)\n",
    "\n",
    "\t#\n",
    "\t#If finished training a model (no more updates, only querying), \n",
    "\t#could do:\n",
    "\t# model.init_sims(replace=True)\n",
    "\t#to trim unneeded model memory = use (much) less RAM.\n",
    "\t#\n",
    "\n",
    "\tprint(\"save the word vector model to disk\")\n",
    "\t#specify path and model's name\n",
    "\tpath = \"/Users/pguo/Desktop/try/\"\n",
    "\tfname = \"wordVectorModel\"\n",
    "\tmodel.save(path+fname)\n",
    "\n",
    "\t#train phrases model\n",
    "\tdimension = 200\n",
    "\tprint(\"train phrases model, word vector's size is %d\" %dimension)\n",
    "\tbigram_transformer = gensim.models.Phrases(sentences)\n",
    "\tmodel = word2vec.Word2Vec(bigram_transformer[sentences], workers=num_workers,\n",
    "\t\t\t\t\t\t\t  size=dimension, min_count=min_count,\n",
    "\t\t\t\t\t\t\t  window=window_size, sample=downsampling, sg = 1)\n",
    "\n",
    "\tprint(\"save the phrases vector model to disk\")\n",
    "\t#specify path and model's name\n",
    "\tpath = \"/Users/pguo/Desktop/try/\"\n",
    "\tfname = \"phrasesVectorModel\"\n",
    "\tmodel.save(path+fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def explore(path):\n",
    "\t#load word vector model\n",
    "\tprint(\"Load Word Vector Model\")\n",
    "\tfname = \"wordVectorModel\"\n",
    "\tmodel = word2vec.Word2Vec.load(path+fname)\n",
    "\t\n",
    "\t#number of words, number of features\n",
    "\tprint(\"show words vector's shape\")\n",
    "\tprint(model.syn0.shape)\n",
    "\n",
    "\tprint(\"access individual word vector\")\n",
    "\tprint(\"for example, the word vector of 'best'\")\n",
    "\tprint(model[\"best\"])\n",
    "\n",
    "\tprint(\"show the word that is most dissimilar from the others\")\n",
    "\tword_set = \"best worst fine London\"\n",
    "\tmost_dissimilar = model.doesnt_match(word_set.split()) + '\\n'\n",
    "\tprint(\"for example, words are %s, the most dissimilar word is %s\" %(word_set, most_dissimilar))\t\n",
    "\n",
    "\t#do more complex queries like analogies such as: king - man + woman = queen \n",
    "\tprint(\"Show most similar words and corresponding distance to the given analogies\")\n",
    "\t#indicate positive words\n",
    "\tpos = ['best', 'refreshing']\n",
    "\t#indicate negative words\n",
    "\tneg = ['worst']\n",
    "\tprint(\"for example, the given positive words are %s, the given negative word is %s\" %(pos, neg))\n",
    "\tmost_similar = model.most_similar(positive=pos, negative=neg)\n",
    "\tprint(most_similar)\n",
    "\n",
    "\tprint(\"show the similarity between two words\")\n",
    "\tword1 = \"worst\"\n",
    "\tword2 = \"best\"\n",
    "\tsimilar = model.similarity(word1, word2)\n",
    "\tprint(\"for example, the similarity between %s and %s is %d\" %(word1, word2, similar))\n",
    "    \n",
    "\tprint(\"test model's accuracy here\")\n",
    "\t#test the quality of the word vectors\n",
    "\t#The accuracy depends heavily on the amount of the training data.\n",
    "\t#read the evaluation file, get it at:\n",
    "\t#https://code.google.com/archive/p/word2vec/source/default/source\n",
    "\tquestions = \"questions-words.txt\"\n",
    "\tevals = open(path+questions, 'r').readlines()\n",
    "\tnum_sections = len([l for l in evals if l.startswith(':')])\n",
    "\tprint('total evaluation sentences: {} '.format(len(evals) - num_sections))\n",
    "\t#test accuracy of model\n",
    "\taccuracy = model.accuracy(path+questions)\n",
    "\tsum_corr = len(accuracy[-1]['correct'])\n",
    "\tsum_incorr = len(accuracy[-1]['incorrect'])\n",
    "\ttotal = sum_corr + sum_incorr\n",
    "\tpercent = lambda a: a / total * 100\n",
    "\tprint('Total sentences: {}, Correct: {:.2f}%, Incorrect: {:.2f}%'.format(total, percent(sum_corr), percent(sum_incorr)))\n",
    "\n",
    "\t#phrases mode\n",
    "\tprint(\"Load phrases model\")\n",
    "\tfname = \"phrasesVectorModel\"\n",
    "\tmodel = word2vec.Word2Vec.load(path+fname)\n",
    "\n",
    "\t#indicate a given word\n",
    "\tgiven = 'los_angeles'\n",
    "\tprint(\"Show most similar words and corresponding distance to the given phrase\")\n",
    "\tprint(\"for example, the given phrase is %s\" %given)\t\n",
    "\tsimilar_word = model.similar_by_word(given)\n",
    "\tprint(similar_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the imdb reviews data and clean the data\n",
      "Data loaded and cleaned.\n",
      "train word vector\n",
      "dimension: word vector dimensionality\n",
      "min_count: any word that does not occur at least this many times across all documents is ignored\n",
      "number of threads to run in parallel\n",
      "window size\n",
      "Training model\n",
      "save the word vector model to disk\n",
      "train phrases, wordvector's size is 200\n",
      "save the phrases vector model to disk\n",
      "explore the model\n",
      "Load Word Vector Model\n",
      "show words vector's shape\n",
      "(30321, 200)\n",
      "access individual word vector\n",
      "for example, the word vector of 'best'\n",
      "[ -2.62199044e-01  -5.76151550e-01   4.25246619e-02  -2.76462585e-01\n",
      "   1.17840350e-01   1.97694078e-01  -3.49061370e-01  -3.37192357e-01\n",
      "   4.45775926e-01  -6.84153214e-02  -1.86107799e-01  -3.09626818e-01\n",
      "   4.64470506e-01  -1.61453173e-01  -7.11624138e-03   2.54027963e-01\n",
      "  -1.90099418e-01  -3.91215533e-02  -5.66844583e-01  -3.25414926e-01\n",
      "  -2.38946527e-01  -1.70620799e-01   3.40528935e-02  -2.18186274e-01\n",
      "  -1.54683143e-02   4.58015144e-01   2.78599441e-01  -3.62772942e-01\n",
      "  -2.38401920e-01  -1.23092800e-01  -1.28488541e-01  -2.91003615e-01\n",
      "  -1.73418000e-01   2.00482428e-01  -4.39274870e-03  -4.88240808e-01\n",
      "   2.03626156e-01   9.44861323e-02   2.65914291e-01   1.84518233e-01\n",
      "   6.66406676e-02   7.66901553e-01   1.19279265e-01  -1.75016269e-01\n",
      "  -2.36780182e-01  -1.91001251e-01  -3.88042301e-01  -5.42079806e-02\n",
      "   3.99252087e-01  -1.10345423e-01  -5.97108901e-02  -1.05539657e-01\n",
      "  -5.71543649e-02   1.35617986e-01  -5.25686324e-01  -4.07280996e-02\n",
      "   1.55159906e-01  -4.50451940e-01  -5.89103043e-01  -4.63584363e-01\n",
      "   3.12296841e-02   4.49722081e-01  -8.23531449e-02   9.07291621e-02\n",
      "   4.28419471e-01  -4.48929399e-01  -5.62635250e-03  -1.39873698e-01\n",
      "  -1.85308531e-01   1.04713321e-01  -1.01808188e-02   5.29955268e-01\n",
      "   4.14229453e-01  -6.62327886e-01  -2.15482220e-01   2.71094441e-01\n",
      "   1.25756368e-01  -9.82829034e-02  -2.04680920e-01   1.56115480e-02\n",
      "  -1.07522272e-01   2.35320926e-01   5.56013398e-02  -8.63791481e-02\n",
      "  -1.06286228e-01   4.45231318e-01   1.56074077e-01   2.95958161e-01\n",
      "  -5.77465482e-02  -1.46114081e-01  -7.80085549e-02  -1.79064110e-01\n",
      "   1.30890787e-01   2.67855767e-02   1.08951367e-01   1.40292212e-01\n",
      "  -4.47005868e-01  -2.71983724e-02   1.42796189e-01  -1.49691388e-01\n",
      "   1.85095102e-01  -3.81053686e-02   1.58579439e-01   3.67290527e-01\n",
      "  -8.67852196e-02   3.31667572e-01   3.22817892e-01   3.35099697e-01\n",
      "  -1.36284575e-01   1.22405402e-01   7.07766935e-02  -5.34274429e-02\n",
      "  -4.17835712e-01   3.83122712e-02  -3.20637450e-02   2.77847022e-01\n",
      "   2.23165169e-01  -3.08827875e-05   1.13920324e-01  -3.01342960e-02\n",
      "   1.04818054e-01   2.10033000e-01   5.92923939e-01   1.49086416e-01\n",
      "   8.85923207e-02   4.22435664e-02  -1.00153849e-01   8.73965472e-02\n",
      "   3.67014498e-01   4.04302031e-01   7.20160604e-02  -2.56392002e-01\n",
      "  -9.27070826e-02   1.51542142e-01   1.01291515e-01   3.88730109e-01\n",
      "   7.76414946e-02   8.94711688e-02  -2.87681252e-01   1.47311330e-01\n",
      "   3.02951276e-01  -4.85518396e-01   3.36547866e-02  -1.71232700e-01\n",
      "   2.10483372e-01  -2.83204287e-01  -8.45716968e-02   9.83356759e-02\n",
      "  -5.32595739e-02   9.24056619e-02   4.18495148e-01   6.20336421e-02\n",
      "   3.68088990e-01   6.52938038e-02   1.30483717e-01  -3.46793324e-01\n",
      "  -2.11805716e-01  -1.24569787e-02   4.36592996e-02  -2.01601669e-01\n",
      "   1.91653728e-01  -3.12511563e-01   8.72696713e-02   1.29934520e-01\n",
      "   1.47770807e-01   3.81076694e-01   5.42690217e-01   2.56907463e-01\n",
      "   8.70703161e-02   3.19903672e-01   2.37009421e-01   4.44611087e-02\n",
      "  -1.38480157e-01  -1.65564984e-01  -3.23639475e-02   4.00557369e-01\n",
      "   4.59313840e-01   1.27185836e-01   2.64170170e-02   2.62535103e-02\n",
      "   2.11295739e-01   2.56747127e-01  -2.36969188e-01   5.59370145e-02\n",
      "   2.55547881e-01   3.85479510e-01   1.30868256e-01  -6.04918636e-02\n",
      "   2.05547705e-01   2.06570979e-02   2.48286977e-01  -2.23729625e-01\n",
      "  -2.48087034e-01  -4.10702266e-02  -4.00778890e-01   3.73855717e-02\n",
      "  -4.27706242e-01  -3.64742726e-01   5.07319570e-02  -1.26171947e-01]\n",
      "show the word that is most dissimilar from the others\n",
      "for example, words are best worst fine London, the most dissimilar word is fine\n",
      "\n",
      "Show most similar words and corresponding distance to the given analogies\n",
      "for example, the given positive words are ['best', 'refreshing'], the given negative word is ['worst']\n",
      "[('lighthearted', 0.5745251178741455), ('heartwarming', 0.5262370705604553), ('wholesome', 0.5232752561569214), ('insightful', 0.5128545761108398), ('delightful', 0.5104857683181763), ('unique', 0.5052674412727356), ('rewarding', 0.5010650753974915), ('informative', 0.5008149147033691), ('somber', 0.5001655220985413), ('melancholic', 0.4972394108772278)]\n",
      "show the similarity between two words\n",
      "for example, the similarity between worst and best is 0\n",
      "test model's accuracy here\n",
      "total evaluation sentences: 19544 \n",
      "Total sentences: 9837, Correct: 14.12%, Incorrect: 85.88%\n",
      "Load phrases model\n",
      "Show most similar words and corresponding distance to the given phrase\n",
      "for example, the given phrase is los_angeles\n",
      "[('northern', 0.8060599565505981), ('oregon', 0.8037513494491577), ('district', 0.7950667142868042), ('austria', 0.7923411130905151), ('southwest', 0.790042519569397), ('1920s', 0.787814736366272), ('mexico', 0.7856687307357788), ('suburbs', 0.7844173312187195), ('19th_century', 0.7840701937675476), ('mainland', 0.7833410501480103)]\n"
     ]
    }
   ],
   "source": [
    "#main\n",
    "#specify the path of datasets\n",
    "neg_path = \"/Users/pguo/Desktop/try/aclImdb/train/neg\"\n",
    "pos_path = \"/Users/pguo/Desktop/try/aclImdb/train/pos\"\n",
    "\n",
    "#use training data create input of word2vector\n",
    "sentences = createInput(neg_path, pos_path)\n",
    "\n",
    "#\n",
    "#What word2vec does is to represent each word as a vector in a form \n",
    "#that reflects how close words occur. \n",
    "#The closer the words tend to occur in a text the more similar the vectors will be.\n",
    "#\n",
    "\n",
    "#use word2vec train model\n",
    "trainModel(sentences)\n",
    "\n",
    "#display what model can do\n",
    "print(\"explore the model\")\n",
    "#model's path\n",
    "path = \"/Users/pguo/Desktop/try/\"\n",
    "explore(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
